import argparse, gzip, sys
import numpy as np
import matplotlib
matplotlib.use('agg')
import matplotlib.pyplot as plt


def parse_args():  # handle user arguments
	parser = argparse.ArgumentParser(description='Create pileups from .paf read-to-read mapping and fastq reads.')
	parser.add_argument('--mapping', required=True, help='Path to the .paf file of read-to-read mappings.')
	parser.add_argument('--reads', required=True, help='Path to the .fastq reads file.')
	args = parser.parse_args()
	return args


def process_reads(reads):  # using fastq file, create dict mapping read names to sequences
	reads_file = gzip.open(reads, "r")
	reads_dict, accept, current = {}, False, ''
	limit, count = 10000, 0  # TEMP
	for line in reads_file:
		if line.startswith('@'):
			current = line[1:].split(' ')[0]
			accept = True
		elif accept:
			reads_dict[current] = line.strip().upper()
			accept = False
			count += 1
			if count > limit:
				break
	reads_file.close()
	return reads_dict


def select_top(matches):
	return matches


def make_pileup(readname, matches, reads_dict):
	encodings = {'A': 250.0, 'C': 200.0, 'G': 150.0, 'T': 100.0}
	pileup = []
	read, seq = reads_dict[readname], []
	readlen, maxlen = len(read), len(read)
	for ch in read:
		seq.append(encodings[ch])
	pileup.append(seq)

	for match in matches:
		seq = []
		for i in range(match[2]):
			seq.append(0.0)  # no info for this base because the target read does not map here
		segment = reads_dict[match[0]][match[4]:match[5]+1]
		for ch in segment:
			seq.append(encodings[ch])
		#for i in range(match[3], readlen):
		while len(seq) < readlen:
			seq.append(0.0)  # again, no info
		if len(seq) > maxlen:
			maxlen = len(seq)
		pileup.append(seq)

	for line in range(len(pileup)):
		pileup[line].extend([0 for i in range(maxlen - len(pileup[line]))])

	return np.array(pileup)


def visualize(reads_list, pileups):
	for p in range(len(pileups)):
		plt.imsave(reads_list[p]+'.png', pileups[p], cmap='gray', vmin=0, vmax=255)
	return


def main():
	limitreads, count = 2, 0  # TEMPORARILY limit number of reads to process for testing purposes 
	window_size, top = 100, 10
	args = parse_args()
	mapfile = gzip.open(args.mapping, 'r')
	reads_dict = process_reads(args.reads)

	prev_read, reads_list, matches, pileups = '', [], [], []
	for line in mapfile:
		splits = line.split('\t')
		cur_read = splits[0]

		if cur_read != prev_read:
			count += 1
			if count > limitreads:
				break

			if prev_read == '':
				prev_read = cur_read
			else:
				prev_read = cur_read
				matches.sort(key=lambda x: x[1], reverse=True)
				#matches = select_top(matches)
				pileups.append(make_pileup(prev_read, matches, reads_dict))
				reads_list.append(prev_read)
				matches = []

		# store target name, match %, and start & end positions for query and target
		if splits[5] not in reads_dict:
			continue
		matches.append([splits[5], float(splits[9])/float(splits[10]), int(splits[2]), int(splits[3]), int(splits[7]), int(splits[8])])

	# Now process the last read
	#matches = select_top(matches)
	pileups.append(make_pileup(cur_read, matches, reads_dict))
	reads_list.append(cur_read)

	mapfile.close()
	visualize(reads_list, pileups)


if __name__ == "__main__":
	main()
#